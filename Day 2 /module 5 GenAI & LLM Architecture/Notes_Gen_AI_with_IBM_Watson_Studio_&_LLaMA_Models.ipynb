{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Notes: Gen AI with IBM Watson Studio & LLaMA Models\n",
        "\n",
        "#1. What is IBM Watson Studio / watsonx.ai?\n",
        "\n",
        "> Watson Studio is IBM‚Äôs cloud platform for AI development, notebooks, model deployment, and data workflows.\n",
        "\n",
        "> watsonx.ai is IBM‚Äôs foundation model service that provides access to multiple enterprise LLMs and open-source LLMs (LLaMA, Granite, Mistral, etc.).\n",
        "\n",
        "> Together, they allow you to build, test, and deploy generative AI apps inside a managed environment.\n",
        "\n",
        "#2. What are LLaMA Models?\n",
        "\n",
        "> LLaMA (Large Language Model Meta AI) is an open-source family of LLMs created by Meta.\n",
        "\n",
        "> IBM offers LLaMA 3 and LLaMA 3.1 variants inside watsonx.ai.\n",
        "\n",
        "These models support:\n",
        "\n",
        ">>Text generation\n",
        "\n",
        ">>Chat / instruction following\n",
        "\n",
        ">>Code generation\n",
        "\n",
        ">>Multimodal Q&A (vision-instruct variants)\n",
        "\n",
        "#Common LLaMA Models on Watsonx\n",
        "\n",
        "1.LLaMA 3 8B / 11B Instruct ‚Äî fastest, low compute cost\n",
        "\n",
        "2.LLaMA 3 70B Instruct ‚Äî higher intelligence, slower\n",
        "\n",
        "3.LLaMA 3 Vision-Instruct ‚Äî supports image + text\n",
        "\n",
        "#3. Why Use Watson Studio for Gen AI?\n",
        "\n",
        ">Fully managed compute environment\n",
        "\n",
        ">No model downloads required\n",
        "\n",
        ">Built-in notebooks with GPU/CPU\n",
        "\n",
        ">Enterprise-level security & governance\n",
        "\n",
        ">Direct SDK support for:\n",
        "\n",
        ">>LLaMA inference\n",
        "\n",
        ">>Embeddings generation\n",
        "\n",
        ">>Prompt templates\n",
        "\n",
        ">>RAG workflows\n",
        "\n",
        "#4. Key Components Students Should Know\n",
        "‚úîÔ∏è 1. Watsonx.ai API / SDK\n",
        "\n",
        "-- Use the ibm-watsonx-ai Python SDK for:\n",
        "\n",
        ">Running prompts\n",
        "\n",
        ">Chat-style conversations\n",
        "\n",
        ">Streaming outputs\n",
        "\n",
        ">Setting generation parameters (tokens, temperature, etc.)\n",
        "\n",
        "‚úîÔ∏è 2. Credentials\n",
        "\n",
        "-- You need:\n",
        "\n",
        "> IAM API Key\n",
        "\n",
        "> Project ID\n",
        "\n",
        "> Endpoint URL\n",
        "\n",
        "‚úîÔ∏è 3. ModelInference class\n",
        "\n",
        "Main class used to run models:\n",
        "\n",
        "```\n",
        "ModelInference(model_id, params, credentials, project_id)\n",
        "\n",
        "```\n",
        "‚úîÔ∏è 4. Generation Parameters\n",
        "\n",
        "> max_new_tokens ‚Üí output length\n",
        "\n",
        "> temperature ‚Üí randomness\n",
        "\n",
        "> top_k, top_p ‚Üí sampling control\n",
        "\n",
        "> return_full_text ‚Üí include prompt + output\n",
        "\n",
        "# 5. Working with LLaMA on Watson Studio\n",
        "\n",
        "üí° Modes of Operation\n",
        "\n",
        "1.Prompt Completion\n",
        "\n",
        "Example: summarize text, explain code, create stories.\n",
        "\n",
        "2.Chat Mode\n",
        "\n",
        "Multi-turn dialogues using system / user roles.\n",
        "\n",
        "3.Structured Output\n",
        "\n",
        "JSON generation for analysis apps.\n",
        "\n",
        "4.Vision Mode (if enabled)\n",
        "\n",
        "LLaMA Vision-Instruct for image reasoning.\n",
        "\n",
        "#6. Basic Workflow for Students\n",
        "\n",
        "Step 1 ‚Äî Start a Notebook\n",
        "\n",
        "Create a new notebook inside Watson Studio project.\n",
        "\n",
        "Step 2 ‚Äî Install SDK\n",
        "\n",
        "```\n",
        "!pip install ibm-watsonx-ai --quiet\n",
        "```\n",
        "\n",
        "Step 3 ‚Äî Set Credentials\n",
        "\n",
        "> Add IAM API Key\n",
        "\n",
        "> Add Watsonx endpoint\n",
        "\n",
        "> Add Project ID\n",
        "\n",
        "Step 4 ‚Äî Initialize Model\n",
        "\n",
        "> Pick a LLaMA model by ID.\n",
        "\n",
        "Step 5 ‚Äî Run Prompts\n",
        "\n",
        "Use:\n",
        "\n",
        "> agenerate() for single prompts\n",
        "\n",
        "> achat() for chat interactions\n",
        "\n",
        "#‚úîÔ∏è Short Summary For Revision\n",
        "\n",
        "Watson Studio + watsonx.ai is IBM‚Äôs Gen AI platform\n",
        "\n",
        "LLaMA models are available as ready-to-use foundation models\n",
        "\n",
        "Students interact via Python SDK using ModelInference\n",
        "\n",
        "Supports text, chat, JSON, and (optionally) vision workflows\n",
        "\n",
        "Key skills: prompting, parameters, model selection, API usage"
      ],
      "metadata": {
        "id": "YivEWZBybjHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB-V0pNwbh7t"
      },
      "outputs": [],
      "source": []
    }
  ]
}