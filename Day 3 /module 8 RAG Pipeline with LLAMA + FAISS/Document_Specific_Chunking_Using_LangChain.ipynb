{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Document Specific Chunking Using LangChain\n",
        "\n",
        "#✅ Objective\n",
        "We’ll:\n",
        "\n",
        "Load different documents (from string for simplicity)\n",
        "\n",
        "Use custom chunking logic based on document content\n",
        "\n",
        "Use langchain.text_splitter to chunk each document differently"
      ],
      "metadata": {
        "id": "yqWqww0_fSjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UApXlwBve8d2",
        "outputId": "b62b28fa-532a-46ce-ce75-cf780bf58dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: langchain-text-splitters\n",
            "Successfully installed langchain-text-splitters-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Practical: Document-Specific Chunking with LangChain"
      ],
      "metadata": {
        "id": "L2W-buWtfayt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Create sample documents with metadata\n",
        "# ---------------------------\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"Chapter 1: Intro\\n\" + \"This is a long paragraph \" * 10,\n",
        "        metadata={\"type\": \"book\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Meeting Notes:\\n- Discussed marketing\\n- Planned Q3 goals\",\n",
        "        metadata={\"type\": \"meeting_notes\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Name, Age, Address\\nJohn, 32, NY\\nJane, 28, CA\\nJake, 35, TX\",\n",
        "        metadata={\"type\": \"csv_like\"}\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "f_mqhdYYe_YK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 2: Define Custom Chunker Based on Document Type"
      ],
      "metadata": {
        "id": "JLF4bRHmfd1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_chunk_document(doc: Document):\n",
        "    doc_type = doc.metadata.get(\"type\", \"default\")\n",
        "\n",
        "    if doc_type == \"book\":\n",
        "        # Use RecursiveCharacterTextSplitter for long narrative text\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "    elif doc_type == \"meeting_notes\":\n",
        "        # Use newline split for bullet-style or notes\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=50, chunk_overlap=10)\n",
        "    elif doc_type == \"csv_like\":\n",
        "        # Use comma separator to split rows/entries\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1, chunk_overlap=0)\n",
        "    else:\n",
        "        # Default to line-based split\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=100, chunk_overlap=20)\n",
        "\n",
        "    return splitter.split_documents([doc])"
      ],
      "metadata": {
        "id": "Wm0iEYVxfBgY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 3: Apply Document-Specific Chunking"
      ],
      "metadata": {
        "id": "O3rp1nNVfgeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each document with its own chunking strategy\n",
        "chunked_docs = []\n",
        "for doc in docs:\n",
        "    chunks = custom_chunk_document(doc)\n",
        "    chunked_docs.extend(chunks)\n",
        "\n",
        "# Print results\n",
        "for i, chunk in enumerate(chunked_docs):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(chunk.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNnS3LpXfG0n",
        "outputId": "e42bb5f2-7040-430c-acc4-940ffc6e25a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 18, which is longer than the specified 1\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 12, which is longer than the specified 1\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 12, which is longer than the specified 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Chunk 1 ---\n",
            "Chapter 1: Intro\n",
            "\n",
            "--- Chunk 2 ---\n",
            "This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph\n",
            "\n",
            "--- Chunk 3 ---\n",
            "This is a long paragraph This is a long paragraph This is a long paragraph This is a long paragraph\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Meeting Notes:\n",
            "- Discussed marketing\n",
            "\n",
            "--- Chunk 5 ---\n",
            "- Planned Q3 goals\n",
            "\n",
            "--- Chunk 6 ---\n",
            "Name, Age, Address\n",
            "\n",
            "--- Chunk 7 ---\n",
            "John, 32, NY\n",
            "\n",
            "--- Chunk 8 ---\n",
            "Jane, 28, CA\n",
            "\n",
            "--- Chunk 9 ---\n",
            "Jake, 35, TX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-ZEqyDwfIYv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}